Web Automation - 
    -Create WebDriver instance
    -Go to a web page
    -Locate the object 
    -Perform the action on the object
    -Anticipate browser response to action
    -Record results using framework
    -Quit the browser and WebDriver

The detailed steps can be found here - 
            https://www.browserstack.com/guide/python-selenium-to-run-web-automation-test

You need to download the required driver for the browser you're using so that the commands are compatible.

Initialize an instance of the webdriver and use the .get method to navigate to the required website.
Perform the operations and then close the driver. this is the gist of web automation.

Here, our contents are in a tabular format. 
So we get the number of rows and columns and iterate through it. use a for loop if you know how many entries are there, or use a while if you don't.

the website is written mainly using angularjs,

conventionally for html, we use the following steps.
    1. find the table, and through the table access the elements and the link 
    2. go through each row of the table,
    3. save the necessary information in an excel sheet or any other format.
    4. once we are through with the table, we find the next button ( if its enabled, there are more pages, if disabled, we have reached the last page)

but with selenium alone, it is not possible to automate angularjs pages as there are a few extra controllers in angular like ng-repeat
an alternative is to use protractor which is built on selenium (java script) 
    or we can use the ngWebDriver api (java,).


Since the table is made using angular, it was way easier to just copy the contents and paste them into an excel file. 
    All the data from the table gets copied and automatically gets formatted by excel. 

right now, we just have the table data, now we need to go through the data and download the pdfs in the links.

To download the pdfs, we can use some built in packages in python.
    1. urllib
    2. wget
    3. response

We can save as a custom file name, in the required file format.
All the required information will be saved in the file name. 
All courses are 9 day courses.
hence I have only included the start date in the file name.

excel.py
reads the data from the excel file and downloads the files(pdf) at the respective link.
saves the pdf as (titleCode_fromDate_numParticipants_numResourcePersons.pdf)
There is a small change in the names of a few programs in some years which are actually the same program. Hence to keep the code same, ive sorted the letters in the code.
also saves all the codes and names of the programs in a txt file available in the folder for reference.

in the website, there are exactle 14 links that are missing, these are files that are skipped.

//TODO:
go through pdfs to extract details.
Details can be extracted using tools, such as adobe/ acrobat.
    1. Use Acrobat (just realized its paid :( , but has a lot of options)
    2. Use the inbuilt power query option in excel. (hard to customize if pdf is not in correct format.)
        #Table.TransformColumnTypes(Table003,{{"Column1", Int64.Type}, {"Column2", type text}, {"Column3", Int64.Type}, {"Column4", Int64.Type}})
        #Table.RenameColumns(#"Changed Type",{{"Column2", "NAMES"}, {"Column1", "Sl. No."},{"Column3","Before#(lf)training"},{"Column4","After#(lf)Training"}})
    3. Using WPS office to convert pdf to excel.


Converting pdfs to excel has been challenging since the format of the pdf was unique. Usually, it should be possible to very easily conver the pdf to excel using inbuilt excel utility,
but the interface and the way excel extracted data from multi page pdfs was very tough to use. Hence I started looking for other software to solve this issue.

Acrobat is very simple to use and can perform the task easily, but it is a paid tool for multiple files.

WPS office is another free option, but only if the number of pages in the pdf is not more than 3. 

Since the pdfs have before/after scores, we need to get the names, address and scores in a readable format.

1. maybe we can make a new sheet for each pdf. (a sheet will have name, address and scores)
2. since the number of sheets is more, it would be easier to get the data in frames from excel directly, so it would be better to keep the files in excel format


A district is an administrative division of an Indian state or territory. 
Most of the rows have districts as part of their address but some of them have cities, so it would be better to see representation by using the last word in the address as the identifier.

writing a script to seperate out the name from the address and store it in a different column

writing a python script using openpyxl to access excel data, which are stored as objects. makes life easier and more organized.

Some teachers have a designation as well as an address and some others don't have this, so I think I can count backwards and if designation isn't there, I will keep the space blank.
I could choose to omit the data field completely but it might come in handy for someone else so I will incorporate that as well.

Apart from the districts being at the end, none of the other addresses are in a particular format and each have different intricacies, so it is better if i just stick to extracting districts as they are the last words in the address.

The best way to make sure that all the districts are spell proof, was to find the count of all the districts having same names, and the count of number of people from each of the districts.
So wrote a script to collect the count of all the districts from the excel sheets and store it in a txt file.
I manually checked the names for duplicates and made a note to correct them. 
Once that was done, I crossed referenced the names of all the districts to check for any spelling errors, and if there were, I went through the dataset and checked them manually.

I went through the count file, and saw that most of the district names entered were not part of the official names from Karnataka website,
I took the official names from the website and made the corrections in the excel files in order to generalize the names for all the entries.
We can see a before and after result of the count file, I have stored both in different files.

Fixed most of the data but there are still a lot of bad addresses or bad districts. Fixing them manually is taking quite a long time.
But the scripts written all run in a matter of seconds so that has made my job a little easier.

And we then get data which was clean with the count of districts and the number of people represented from each of the districts.

while correcting the data manually, came across a file that wasnt extracted successfully. High School Mathematics Teachers Name List (04-12-2020 to 13-12-2020)


